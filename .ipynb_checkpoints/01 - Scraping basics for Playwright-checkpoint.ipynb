{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping basics for Playwright\n",
    "\n",
    "This notebook is a combination of small scraping techniques along with how to use Playwright. Along with the class notes, the [scraping section](https://jonathansoma.com/everything/scraping/) on my Everything I Know site might be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import what you need to use Playwright, and start up a new browser to use for scraping. \n",
    "\n",
    "> If you end up opening a lot of Chromes/Chromiums, shutting down the Python kernel with the stop button is an easy way to make them go away! You'll have to re-run your notebook, but at least you won't have sixty icons in your dock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = await browser.new_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping by class\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/by-class.html using their **class name**, printing out the title, subhead, and byline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/columbia/interactive-scrape/by-class.html' request=<Request url='https://jonathansoma.com/columbia/interactive-scrape/by-class.html' method='GET'>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await paxge.goto(\"http://jonathansoma.com/columbia/interactive-scrape/by-class.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><html><head><script>\\n    const html = `\\n<h1 class=\"title\">How to Scrape Things</h1>\\n<h3 class=\"subhead\">Probably using Playwright</h3>\\n<p class=\"byline\">By Jonathan Soma</p>\\n`\\n\\nsetTimeout(() => {\\n    console.log(html)\\n    document.querySelector(\\'body\\').innerHTML = html\\n}, 250)</script>\\n</head><body>\\n<h1 class=\"title\">How to Scrape Things</h1>\\n<h3 class=\"subhead\">Probably using Playwright</h3>\\n<p class=\"byline\">By Jonathan Soma</p>\\n</body></html>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = await page.content()\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup_doc = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to Scrape Things'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_doc.find(class_='title').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping using a single tag\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/by-list.html, creating a dictionary out of the title, subhead, and byline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'How to Scrape Things',\n",
       " 'subhead': 'Probably using Playwright',\n",
       " 'byline': 'By Jonathan Soma'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = soup_doc.find(class_=\"title\").text\n",
    "subhead = soup_doc.find(class_=\"subhead\").text\n",
    "byline = soup_doc.find(class_=\"byline\").text\n",
    "\n",
    "dict_1 = {\n",
    "    'title': title, \n",
    "    'subhead': subhead, \n",
    "    'byline': byline \n",
    "}\n",
    "dict_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waiting\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/by-tag-wait.html just like you above, but use  **wait_for** to wait for the text \"Everything has shown up\" to show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/columbia/interactive-scrape/by-tag-wait.html' request=<Request url='https://jonathansoma.com/columbia/interactive-scrape/by-tag-wait.html' method='GET'>>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)\n",
    "page = await browser.new_page()\n",
    "await page.goto(\"http://jonathansoma.com/columbia/interactive-scrape/by-tag-wait.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><html><head><script>\\n    const html = `<p>How to Scrape Things</p>\\n<p>Probably using Playwright</p>\\n<p>By Jonathan Soma</p>\\n<p>Everything has shown up</p> \\n`\\n\\nlet pieces = html.split(\"\\\\n\")\\n\\nfunction addPiece() {\\n    document.querySelector(\\'body\\').innerHTML = document.querySelector(\\'body\\').innerHTML + pieces.shift()\\n    if(pieces.length > 0) {\\n        setTimeout(addPiece, 250)\\n    } else {\\n        setTimeout(() => {\\n            document.querySelector(\\'body\\').innerHTML = \"\"\\n            pieces = html.split(\"\\\\n\")\\n            setTimeout(addPiece, 1000)\\n        }, 2000)\\n    }\\n}\\n\\nsetTimeout(addPiece, 250)\\n</script>\\n</head><body><p>How to Scrape Things</p><p>Probably using Playwright</p><p>By Jonathan Soma</p><p>Everything has shown up</p> </body></html>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = await page.content()\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forms\n",
    "\n",
    "Display the content of the `h1` tag on http://jonathansoma.com/columbia/interactive-scrape/inputs.html. You'll need to follow the instructions to complete the form first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = await browser.new_page()\n",
    "await page.goto(\"http://jonathansoma.com/columbia/interactive-scrape/inputs.html\")\n",
    "await page.get_by_label(\"The secret is:\").select_option('Open')\n",
    "await page.fill('input[text=\"write cat here\"]', 'cat')  \n",
    "await page.wait_for_selector(\"h1\")\n",
    "html = await page.content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You did it\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "h1 = soup_doc.find(\"h1\")\n",
    "print(h1.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a single table row\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/single-table-row.html, creating a dictionary out of the title, subhead, and byline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td>How to Scrape Things</td>,\n",
       " <td>Probably using Playwright</td>,\n",
       " <td>By Jonathan Soma</td>]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = await browser.new_page()\n",
    "await page.goto(\"http://jonathansoma.com/columbia/interactive-scrape/single-table-row.html\")\n",
    "await page.wait_for_selector(\"td\")\n",
    "html = await page.content()\n",
    "from bs4 import BeautifulSoup\n",
    "soup_doc = BeautifulSoup(html, 'html.parser')\n",
    "html\n",
    "td_tags = soup_doc.find_all(\"td\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'How to Scrape Things',\n",
       " 'subhead': 'Probably using Playwright',\n",
       " 'byline': 'By Jonathan Soma'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = td_tags[0].text\n",
    "subhead = td_tags[1].text\n",
    "byline = td_tags[2].text\n",
    "\n",
    "dict_1 = {\n",
    "    'title': title, \n",
    "    'subhead': subhead, \n",
    "    'byline': byline \n",
    "}\n",
    "dict_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving into a dictionary\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/single-table-row.html, saving the title, subhead, and byline into a single dictionary called `book`.\n",
    "\n",
    "> Don't use pandas for this one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'How to Scrape Things',\n",
       " 'subhead': 'Probably using Playwright',\n",
       " 'byline': 'By Jonathan Soma'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = td_tags[0].text\n",
    "subhead = td_tags[1].text\n",
    "byline = td_tags[2].text\n",
    "\n",
    "book = {\n",
    "    'title': title, \n",
    "    'subhead': subhead, \n",
    "    'byline': byline \n",
    "}\n",
    "book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping multiple table rows\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/multiple-table-rows.html, creating a list of dictionaries. Convert to a pandas dataframe with `pd.json_normalize`. Save it as `output.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "page = await browser.new_page()\n",
    "await page.goto(\"http://jonathansoma.com/columbia/interactive-scrape/multiple-table-rows.html\")\n",
    "\n",
    "await page.wait_for_selector(\"td\")\n",
    "\n",
    "html = await page.content()\n",
    "\n",
    "soup_doc = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "tr_tags = soup_doc.find_all(\"tr\")\n",
    "\n",
    "book_list = []\n",
    "\n",
    "for tr in tr_tags:\n",
    "            td_tags = tr.find_all(\"td\")\n",
    "            title = td_tags[0].text.strip()\n",
    "            subhead = td_tags[1].text.strip()\n",
    "            byline = td_tags[2].text.strip()\n",
    "\n",
    "            book = {\n",
    "                'title': title,\n",
    "                'subhead': subhead,\n",
    "                'byline': byline\n",
    "            }\n",
    "            book_list.append(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subhead</th>\n",
       "      <th>byline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to Scrape Things</td>\n",
       "      <td>Probably using Playwright</td>\n",
       "      <td>By Jonathan Soma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to Scrape Many Things</td>\n",
       "      <td>But, Is It Even Possible?</td>\n",
       "      <td>By Sonathan Joma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The End of Scraping</td>\n",
       "      <td>Let's All Use CSV Files</td>\n",
       "      <td>By Amos Nathanos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title                    subhead            byline\n",
       "0       How to Scrape Things  Probably using Playwright  By Jonathan Soma\n",
       "1  How to Scrape Many Things  But, Is It Even Possible?  By Sonathan Joma\n",
       "2        The End of Scraping    Let's All Use CSV Files  By Amos Nathanos"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.json_normalize(book_list)\n",
    "df.to_csv(\"output.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping an actual table\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/the-actual-table.html using pandas' HTML reading function. Save it as `output.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to Scrape Things</td>\n",
       "      <td>Probably using Playwright</td>\n",
       "      <td>By Jonathan Soma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to Scrape Many Things</td>\n",
       "      <td>But, Is It Even Possible?</td>\n",
       "      <td>By Sonathan Joma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The End of Scraping</td>\n",
       "      <td>Let's All Use CSV Files</td>\n",
       "      <td>By Amos Nathanos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0                          1                 2\n",
       "0       How to Scrape Things  Probably using Playwright  By Jonathan Soma\n",
       "1  How to Scrape Many Things  But, Is It Even Possible?  By Sonathan Joma\n",
       "2        The End of Scraping    Let's All Use CSV Files  By Amos Nathanos"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "page = await browser.new_page()\n",
    "await page.goto(\"http://jonathansoma.com/columbia/interactive-scrape/multiple-table-rows.html\")\n",
    "\n",
    "await page.wait_for_selector(\"td\")\n",
    "\n",
    "html = await page.content()\n",
    "tables = pd.read_html(io.StringIO(html))\n",
    "df.to_csv(\"output.csv\", index=False)\n",
    "df = tables[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `html.parser` vs `html5lib`\n",
    "\n",
    "Here is some good HTML:\n",
    "\n",
    "```python\n",
    "html_good = \"\"\"\n",
    "<h1>This is a title</h1>\n",
    "<h2>This is a subhead</h2>\n",
    "<p>This is a paragraph</p>\n",
    "<p>This is another paragraph</p>\n",
    "\"\"\"\n",
    "\n",
    "Here is some bad HTML:\n",
    "    \n",
    "html_bad = \"\"\"\n",
    "<h1>This is a title\n",
    "<h2>This is a subhead\n",
    "<p>This is a paragraph\n",
    "<p>This is another paragraph\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "When you're using BeautifulSoup, you can use different parsers, including `html.parser`, `html5lib` and `lxml`. Try both the good HTML and bad HTML with each parser and use `print(soup_doc.prettify())` to view the difference.\n",
    "\n",
    "What is different about each one?\n",
    "\n",
    "> You'll need to `pip install` for both html5lib and lxml. Since you aren't important them, they're coming from BeautifulSoup, you'll need to do **Kernel > Restart** and run from the top after installing to have them work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: html5lib in /Users/chivo/.pyenv/versions/3.12.7/lib/python3.12/site-packages (1.1)\n",
      "Requirement already satisfied: six>=1.9 in /Users/chivo/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from html5lib) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /Users/chivo/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from html5lib) (0.5.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: lxml in /Users/chivo/.pyenv/versions/3.12.7/lib/python3.12/site-packages (4.9.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install html5lib\n",
    "! pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parser: html.parser\n",
      "<h1>\n",
      " This is a title\n",
      "</h1>\n",
      "<h2>\n",
      " This is a subhead\n",
      "</h2>\n",
      "<p>\n",
      " This is a paragraph\n",
      "</p>\n",
      "<p>\n",
      " This is another paragraph\n",
      "</p>\n",
      "\n",
      "<h1>\n",
      " This is a title\n",
      " <h2>\n",
      "  This is a subhead\n",
      "  <p>\n",
      "   This is a paragraph\n",
      "   <p>\n",
      "    This is another paragraph\n",
      "   </p>\n",
      "  </p>\n",
      " </h2>\n",
      "</h1>\n",
      "\n",
      "Using parser: html5lib\n",
      "<html>\n",
      " <head>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   This is a title\n",
      "  </h1>\n",
      "  <h2>\n",
      "   This is a subhead\n",
      "  </h2>\n",
      "  <p>\n",
      "   This is a paragraph\n",
      "  </p>\n",
      "  <p>\n",
      "   This is another paragraph\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n",
      "<html>\n",
      " <head>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   This is a title\n",
      "  </h1>\n",
      "  <h2>\n",
      "   This is a subhead\n",
      "   <p>\n",
      "    This is a paragraph\n",
      "   </p>\n",
      "   <p>\n",
      "    This is another paragraph\n",
      "   </p>\n",
      "  </h2>\n",
      " </body>\n",
      "</html>\n",
      "\n",
      "Using parser: lxml\n",
      "<html>\n",
      " <body>\n",
      "  <h1>\n",
      "   This is a title\n",
      "  </h1>\n",
      "  <h2>\n",
      "   This is a subhead\n",
      "  </h2>\n",
      "  <p>\n",
      "   This is a paragraph\n",
      "  </p>\n",
      "  <p>\n",
      "   This is another paragraph\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n",
      "<html>\n",
      " <body>\n",
      "  <h1>\n",
      "   This is a title\n",
      "   <h2>\n",
      "    This is a subhead\n",
      "   </h2>\n",
      "  </h1>\n",
      "  <p>\n",
      "   This is a paragraph\n",
      "  </p>\n",
      "  <p>\n",
      "   This is another paragraph\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html_good = \"\"\"\n",
    "    <h1>This is a title</h1>\n",
    "    <h2>This is a subhead</h2>\n",
    "    <p>This is a paragraph</p>\n",
    "    <p>This is another paragraph</p>\n",
    "    \"\"\"\n",
    "html_bad = \"\"\"\n",
    "    <h1>This is a title\n",
    "    <h2>This is a subhead\n",
    "    <p>This is a paragraph\n",
    "    <p>This is another paragraph\n",
    "    \"\"\"\n",
    "parsers = [\"html.parser\", \"html5lib\", \"lxml\"]\n",
    "\n",
    "for parser in parsers:\n",
    "    print(f\"Using parser: {parser}\")\n",
    "    \n",
    "    soup_good = BeautifulSoup(html_good, parser)\n",
    "    print(soup_good.prettify())\n",
    "\n",
    "    soup_bad = BeautifulSoup(html_bad, parser)\n",
    "    print(soup_bad.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
